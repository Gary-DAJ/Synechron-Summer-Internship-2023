{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "hJjKhEWLmh1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LayoutLM ([huggingface documentation](https://huggingface.co/docs/transformers/model_doc/layoutlm#overview)) is a model for effective pretraining method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. \\\n",
        "In this notebook, a fine-tuned LayoutLM model is used for the task of Document Question Answering on documents such as invoices, forms, Cheques, ID documents, etc. \\\n",
        "It can also perform well on unstructured documents (without clear key-value pairs) like lease agreements, just like nlp question-answering models."
      ],
      "metadata": {
        "id": "NgBTKk5tkZIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ks7R7xZhkKEk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9yw71VouhVU"
      },
      "outputs": [],
      "source": [
        "# Necessary Installations and Imports\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pillow\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForDocumentQuestionAnswering, AutoProcessor\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"magorshunov/layoutlm-invoices\")\n",
        "# processor = AutoProcessor.from_pretrained(\"magorshunov/layoutlm-invoices\")\n",
        "model = AutoModelForDocumentQuestionAnswering.from_pretrained(\"magorshunov/layoutlm-invoices\")\n",
        "\n",
        "from transformers import DocumentQuestionAnsweringPipeline\n",
        "pipe = DocumentQuestionAnsweringPipeline(model=model, framework = \"pt\", tokenizer=tokenizer) #, processor=processor\n",
        "\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!which tesseract\n",
        "tesseract_cmd = (r'/usr/bin/tesseract')\n",
        "\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "1. `up_dpi(image, scale)`: It takes as input 2 parameters- `image`, the image as filepath, `scale`, the factor by which you want to scale the image. \\\n",
        "It returns the resized image `res` as array.\n",
        "2. `get_word_boxes(image, scale)`: It takes as input 2 parameters- `image`, the image as filepath, `scale`, the factor by which you want to scale the image. It performs OCR using Tesseract. \\\n",
        "Currently, the configuration used is `--psm 6`. The configuration can be played around with, to get the best OCR results. A good read on Tesseract page segmentation modes can be found [here](https://pyimagesearch.com/2021/11/15/tesseract-page-segmentation-modes-psms-explained-how-to-improve-your-ocr-accuracy/). \\\n",
        "This functions extracts the word and bounding box coordinates, normalizes it to (1000, 1000) and returns them as a `list[(str)text, ((int)x0, (int)y0, (int)x1, (int)y1)]`. \\\n",
        "The statement `if(d[\"conf\"][i]>60)` sets the minimum confidence score for text to be considered. This helps remove noise caused due to images, few characters of different languages, etc.\n",
        "3. `get_answers(image, questions, scale)`: It takes 3 parameters as input- `image`, the image as filepath, `questions`, a `list[(str)question]`. \\\n",
        "It iterates through `questions` and passes the `image`, i_th question, the word and bounding box information into the document question answering pipeline created in the imports section. \\\n",
        "It returns a list `answers`.\n",
        "4. `get_answers(image, questions, scale)`: Same as `get_answers` except that it returns a `list[(str)answer, (float)confidence score]`. It is sometimes useful to know the confidence score because it is an indicator of a possible incorrect answer."
      ],
      "metadata": {
        "id": "jkuqKYkUmrNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cEJm3VBruno_"
      },
      "outputs": [],
      "source": [
        "def up_dpi(image, scale):\n",
        "    res = cv2.resize(cv2.imread(image), None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC) # scale=2 (default)\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_word_boxes(image, scale):\n",
        "    # image_as_array = cv2.imread(image)\n",
        "    image_as_array = up_dpi(image, scale)\n",
        "    H, W = image_as_array.shape[:2]\n",
        "    # options = \"tesseract sample_images/image2.jpg stdout -l eng --psm 6 -c tessedit_char_whitelist=abcdefghijklmnopqrstuvwxyz0123456789/-\"\n",
        "    d = pytesseract.image_to_data(image_as_array, config=\"--psm 6\", output_type=Output.DICT) # Image.open(image) if image is a filepath  config=options,\n",
        "    n_boxes = len(d['level'])\n",
        "    word_boxes = []\n",
        "    for i in range(n_boxes):\n",
        "        if(d[\"conf\"][i] > 60): # Setting minimum confidence to consider\n",
        "            text = d[\"text\"][i]\n",
        "            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
        "            bbox = (int(x/W*1000), int(y/H*1000), int((x+w)/W*1000), int((y+h)/H*1000))\n",
        "            word_boxes.append((text, bbox))\n",
        "            cv2.rectangle(image_as_array, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "    # cv2_imshow(image_as_array)\n",
        "    # cv2.waitKey(0)\n",
        "    return word_boxes\n",
        "\n",
        "\n",
        "def get_answers(image, questions, scale):\n",
        "    # image->filepath, questions->List\n",
        "    word_boxes = get_word_boxes(image, scale)\n",
        "    n_q = len(questions)\n",
        "    answers = []\n",
        "    for i in range(n_q):\n",
        "        result = pipe([{\"image\": image, \"question\": questions[i], \"word_boxes\":word_boxes}])\n",
        "        ans = result[0][0][\"answer\"]\n",
        "        # score = result[0][0][\"score\"]\n",
        "        answers.append(ans)\n",
        "    return answers\n",
        "\n",
        "def get_answers_scores(image, questions, scale):\n",
        "    # image->filepath, questions->List\n",
        "    word_boxes = get_word_boxes(image, scale)\n",
        "    n_q = len(questions)\n",
        "    answers = []\n",
        "    scores = []\n",
        "    for i in range(n_q):\n",
        "        result = pipe([{\"image\": image, \"question\": questions[i], \"word_boxes\":word_boxes}])\n",
        "        ans = result[0][0][\"answer\"]\n",
        "        score = result[0][0][\"score\"]\n",
        "        answers.append(ans)\n",
        "        scores.append(score)\n",
        "    return list(zip(answers, scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo\n",
        "Used Gradio for the demo\n",
        "- Tab 1: Invoices- \\\n",
        "Click on upload an image. It uploads an image and stores it as filepath (because the functions that are called, take an image as filepath). Set a suitable scale. Click on \"Get Payment Details\" button to extract some key information from the invoice. The \"Process Payment\" button does not work for now.\n",
        "- Tab 2: e-KYC- \\\n",
        "This tab has a chatbot for natural language queries. Upload an image, set a suitable scale, type a question and hit enter. Click on \"Clear\" when you want to clear the chat history.\n",
        "- Tab 3: Cheques- \\\n",
        "Functionality is similar to Invoices tab. It performs decent on handwritten cheques. It almost always gets the A/C No. correctly. Sometimes messes up with the name and amount. The right question asked in the `cheque_answers()` function can get the required answer. Sometimes, fancy font styles in cheques also affects the results.\n",
        "- Tab 4: Forms- \\\n",
        "You can ask all possible questions, in newlines, in the \"Questions\" Textbox. Click the \"Get answers\" button. All the answers with their confidence scores will appear in the \"Answers Textbox. **This tab is good for experimentation.**"
      ],
      "metadata": {
        "id": "kwHabFqfuuZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hszrja2avcl8"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall gradio\n",
        "# !pip install -q gradio --use-deprecated=legacy-resolver\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "qRAS_J_Hv2Ma",
        "outputId": "e0a2e86c-5232-47b7-87fe-2f66a85298c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e98e7e3220d76589bc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e98e7e3220d76589bc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"NOTE: Upload a document image and set a suitable scale factor for accurate OCR\")\n",
        "    with gr.Tab(\"Invoice\"):\n",
        "        image_1 = gr.Image(type=\"filepath\")\n",
        "        number_1 = gr.Number(value=2, label=\"Scale factor\")\n",
        "        button_1 = gr.Button(\"Get Payment Details\")\n",
        "        with gr.Row():\n",
        "            name_1 = gr.Textbox(label=\"Client Name\", interactive=True)\n",
        "            inv_1 = gr.Textbox(label=\"Invoice Number\", interactive=True)\n",
        "            dt_1 = gr.Textbox(label=\"Invoice Date\", interactive=True)\n",
        "            tax_1 = gr.Textbox(label=\"Sales Tax\", interactive=True)\n",
        "            tot_1 = gr.Textbox(label=\"Total\", interactive=True)\n",
        "        dump_button_3 = gr.Button(\"Process Payment\")\n",
        "        def invoice_answers(image, scale):\n",
        "            questions = [\"What is the Client Name?\", \"What is the Invoice Number?\", \"What is the invoice Date?\", \"What is the Sales Tax?\", \"What is the Total?\"]\n",
        "            answers = get_answers(image, questions, scale)\n",
        "            return answers\n",
        "\n",
        "    with gr.Tab(\"e-KYC\"):\n",
        "        with gr.Row():\n",
        "            image_2 = gr.Image(type=\"filepath\")\n",
        "            chatbot_2 = gr.Chatbot()\n",
        "        number_2 = gr.Number(value=2, label=\"Scale factor\")\n",
        "        question_2 = gr.Textbox(label=\"Question\")\n",
        "        clear_2 = gr.ClearButton([question_2, chatbot_2])\n",
        "        def respond_2(image, question, scale, chat_history):\n",
        "            bot_message = get_answers(image, [question], scale)[0]\n",
        "            chat_history.append((question, bot_message))\n",
        "            time.sleep(2)\n",
        "            return \"\", chat_history\n",
        "        question_2.submit(respond_2, [image_2, question_2, number_2, chatbot_2], [question_2, chatbot_2])\n",
        "\n",
        "    with gr.Tab(\"Cheque\"):\n",
        "        image_3 = gr.Image(type=\"filepath\")\n",
        "        number_3 = gr.Number(value=2, label=\"Scale factor\")\n",
        "        button_3 = gr.Button(\"Get Info\")\n",
        "        with gr.Row():\n",
        "            pay_3 = gr.Textbox(label=\"Pay\", interactive=True)\n",
        "            amt_3 = gr.Textbox(label=\"Amount\", interactive=True)\n",
        "            ac_3 = gr.Textbox(label=\"Ac/No.\", interactive=True)\n",
        "            date_3 = gr.Textbox(label=\"Date\", interactive=True)\n",
        "        dump_button_3 = gr.Button(\"Process\")\n",
        "        def cheque_answers(image, scale):\n",
        "            questions = [\"What is the Date?\", \"What is the Pay?\", \"What is the Ac/No?\", \"What is the Amount?\"]\n",
        "            answers = get_answers(image, questions, scale)\n",
        "            return answers\n",
        "\n",
        "    with gr.Tab(\"Form\"):\n",
        "        gr.Markdown(\"Type your questions in the 'Questions' Textbox, each question in a new line\")\n",
        "        with gr.Row():\n",
        "            image_4 = gr.Image(type=\"filepath\")\n",
        "            number_4 = gr.Number(label=\"Scale factor\", value=2)\n",
        "        with gr.Row():\n",
        "            question_4 = gr.Textbox(label=\"Questions\", lines=12)\n",
        "            answer_4 = gr.Textbox(label=\"Answers\", lines=12)\n",
        "        with gr.Row():\n",
        "            button_4 = gr.Button(\"Get answers\")\n",
        "            dump_button_4 = gr.Button(\"Dump\")\n",
        "        def form_answers(image, questions, scale):\n",
        "            q = questions.strip(\"\\n\").split(\"\\n\")\n",
        "            results = get_answers_scores(image, q, scale)\n",
        "            out = \"\"\n",
        "            for i, (ans, sco) in enumerate(results):\n",
        "                out = out + f\"{i+1}. {ans} (score={sco})\\n\"\n",
        "            return out\n",
        "\n",
        "    # with gr.Accordion(\"Open for More!\"):\n",
        "    #     gr.Markdown(\"Look at me...\")\n",
        "\n",
        "    button_1.click(invoice_answers, inputs=[image_1, number_1], outputs=[name_1, inv_1, dt_1, tax_1, tot_1])\n",
        "    button_3.click(cheque_answers, inputs=[image_3, number_3], outputs=[date_3, pay_3, ac_3, amt_3])\n",
        "    button_4.click(form_answers, inputs=[image_4, question_4, number_4], outputs=answer_4)\n",
        "\n",
        "demo.launch() # debug=True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples\n"
      ],
      "metadata": {
        "id": "N_BB1diT05Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"0\")\n",
        "NOTE = \"\"\"\n",
        "These examples can be best tried out with the \"Forms\" Tab to see how many of these questions are answered\n",
        "\"\"\"\n",
        "\n",
        "invoice_0 = {\n",
        "    \"image\": \"https://eswap.global/wp-content/uploads/2021/08/invoice.png\",\n",
        "    \"scale\": 2,\n",
        "    \"questions\": \"\"\"\n",
        "What is the Client Name?\n",
        "What is the Invoice No?\n",
        "What is the Invoice Date?\n",
        "What is the Due Date?\n",
        "What is the Amount of Labor 3hrs?\n",
        "What is the Unit Price of New set of Pedal arms?\n",
        "What is the Subtotal?\n",
        "What is the Sales Tax?\n",
        "What is the Total?\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "invoice_1 = {\n",
        "    \"image\": \"https://docs.bellatrix.solutions/product-integrations/images/sampleinvoice.png\",\n",
        "    \"scale\": 2,\n",
        "    \"questions\": \"\"\"\n",
        "What is the Company?\n",
        "What is the name?\n",
        "What is the address?\n",
        "What is the email?\n",
        "What is the Subtotal?\n",
        "What is the Shipping?\n",
        "What is the Tax?\n",
        "What is the Total?\n",
        "What is the quantity of Cotton Male T-shirt?\n",
        "What is the Unit price of Cotton Male T-shirt?\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "kyc_0 = {\n",
        "    \"image\": \"https://i.imgur.com/aHeN4vj.jpeg\",\n",
        "    \"scale\": 4,\n",
        "    \"questions\": \"\"\"\n",
        "What is the Licence No.?\n",
        "What is the Name?\n",
        "What is the S/W/D?\n",
        "What is the Address?\n",
        "What is the BG?\n",
        "What is the Authorization to Drive?\n",
        "What is the Issue Date?\n",
        "What is the Validity Date?\n",
        "What is the Inv Carr No?\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "kyc_1 = {\n",
        "    \"image\": \"https://upload.wikimedia.org/wikipedia/commons/5/56/Specimen_Personal_Information_Page_South_Korean_Passport.jpg\",\n",
        "    \"scale\": 3,\n",
        "    \"questions\": \"\"\"\n",
        "What is the Type?\n",
        "What is the Issuing country?\n",
        "What is the Passport No.?\n",
        "What is the Surname?\n",
        "What are the Given Names?\n",
        "What is the Nationality?\n",
        "What is the Date of Birth?\n",
        "What is the Personal No.?\n",
        "What is the Sex?\n",
        "What is the Date of Issue?\n",
        "What is the Date of Expiry?\n",
        "What is the Authority?\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "kyc_2 = {\n",
        "    \"image\": \"https://www.immihelp.com/assets/article-images/sample-oci-card-2.jpg\",\n",
        "    \"scale\": 6,\n",
        "    \"questions\": \"\"\"\n",
        "What is the Surname?\n",
        "What is the Given Name?\n",
        "What is the Nationality?\n",
        "What is the Place of Birth?\n",
        "What is the Sex?\n",
        "What is the Date of Birth?\n",
        "What is the Occupation?\n",
        "What is the Place of Issue?\n",
        "What is the Date of Issue?\n",
        "What is the No.?\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Questions 5, 6, 7, 8 get incorrect answers due to improper OCR. Scroll below to find a Gradio demo to check this out.\n",
        "# Try different scales and page segmentation modes\n",
        "form_0 = {\n",
        "    \"image\": \"https://bemoneyaware.com/wp-content/uploads/2019/03/car-insurance-policy.jpg\",\n",
        "    \"scale\": 2.5,\n",
        "    \"questions\": \"\"\"\n",
        "What is the Total Liability Premium?\n",
        "What is the Basic Premium on Vehicle and non-electrical accessories?\n",
        "What is the Aggravation Cover?\n",
        "What is the Net Premium (A+B)?\n",
        "What is the Total Premium Payable?\n",
        "What is the Total Own Damage Premium?\n",
        "What is the Nominee Age?\n",
        "What is the OD Premium the preceding year?\n",
        "Where was this policy signed at?\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "form_1 = {\n",
        "    \"image\": \"https://imgv2-2-f.scribdassets.com/img/document/371202185/original/a6147f7f76/1688705823?v=1\",\n",
        "    \"scale\": 2.5,\n",
        "    \"questions\": \"\"\"\n",
        "What is the Policy No?\n",
        "What is the Prev Policy No?\n",
        "What is the Insured's Name?\n",
        "What is the Helpline No.?\n",
        "What is the Issue Office Name?\n",
        "What is the Service Tax?\n",
        "What is the Total?\n",
        "What is the Total value?\n",
        "What is the Date?\n",
        "What is the Address?\n",
        "    \"\"\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4beXxJZVgQcZ",
        "outputId": "f969dd09-4a34-431c-f134-b2d22819ae5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The following demo is to show how changing the Tesseract page segmentation mode can change OCR results.\n",
        "- I have used psm 6 as the default configuration in the above demo because it worked well in most cases.\n",
        "- If you have a folder full of document images of one type, you can experiment with one image to get the suitable scale and psm for images of that type."
      ],
      "metadata": {
        "id": "j7S129bSFHOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "def get_word_boxes_psm(image, scale, psm=3):\n",
        "    if scale<=0:\n",
        "        scale = 2\n",
        "    image_as_array = up_dpi(image, scale)\n",
        "    H, W = image_as_array.shape[:2]\n",
        "    if psm not in [1,3,4,5,6,7,8,9,10,11,12,13]:\n",
        "        psm = 3\n",
        "    d = pytesseract.image_to_data(image_as_array, config=f\"--psm {psm}\", output_type=Output.DICT)\n",
        "    n_boxes = len(d['level'])\n",
        "    word_boxes = []\n",
        "    for i in range(n_boxes):\n",
        "        if(d[\"conf\"][i] > 60): # Setting minimum confidence to consider\n",
        "            text = d[\"text\"][i]\n",
        "            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
        "            bbox = (int(x/W*1000), int(y/H*1000), int((x+w)/W*1000), int((y+h)/H*1000))\n",
        "            word_boxes.append((text, bbox))\n",
        "            cv2.rectangle(image_as_array, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            cv2.putText(image_as_array, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
        "    # cv2_imshow(image_as_array)\n",
        "    # cv2.waitKey(0)\n",
        "    return image_as_array"
      ],
      "metadata": {
        "id": "Dr9VjMc4UVOZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo2:\n",
        "    with gr.Row():\n",
        "        image_x = gr.Image(type=\"filepath\")\n",
        "        output_x = gr.Image()\n",
        "    with gr.Row():\n",
        "        scale_x = gr.Number(value=2, label=\"Scale factor\")\n",
        "        psm_x = gr.Slider(1,13,3,step=1,label=\"psm\")\n",
        "    put_text = gr.Button(\"OCR\")\n",
        "    put_text.click(fn=get_word_boxes_psm, inputs=[image_x, scale_x, psm_x], outputs=output_x)\n",
        "demo2.launch() # debug=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "HuxaQ73edbMQ",
        "outputId": "010470bf-f33e-4f56-e11d-46aa7c1ccea3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3af7d08adb8eaba99a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3af7d08adb8eaba99a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another idea is to use langchain to include Generative AI in the chatbot. You can first use the \"Forms\" mode to get all possible questions and answers and include it under the \"Summary of the conversation\". Then, under \"Current conversation\", you can enter a question that can be answered by Generative AI."
      ],
      "metadata": {
        "id": "gwvJIQxFGF7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "1cvzDelYsweB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory, CombinedMemory, ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "BlMWf2QPsnEf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history_lines\",\n",
        "    input_key=\"input\",\n",
        "    k=1\n",
        ")\n",
        "\n",
        "summary_memory = ConversationSummaryMemory(llm=OpenAI(), input_key=\"input\")\n",
        "# Combined\n",
        "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
        "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "Summary of conversation:\n",
        "{history}\n",
        "Current conversation:\n",
        "{chat_history_lines}\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\", \"chat_history_lines\"], template=_DEFAULT_TEMPLATE\n",
        ")\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        "    prompt=PROMPT\n",
        ")"
      ],
      "metadata": {
        "id": "gjy4eHFcsrOF"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}